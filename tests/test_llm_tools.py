import asyncio
import os
import sys
import json
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

# ==========================================
# 1. ç¯å¢ƒä¸ Mock è®¾ç½® (ä¸ºäº†è®©è„šæœ¬èƒ½ç‹¬ç«‹è¿è¡Œ)
# ==========================================

# æ¨¡æ‹Ÿ Logger
class MockLogger:
    def debug(self, msg): print(f"[DEBUG] {msg}")
    def info(self, msg): print(f"[INFO] {msg}")
    def warning(self, msg): print(f"[WARN] {msg}")
    def error(self, msg, exc_info=False): print(f"[ERROR] {msg}")

# æ¨¡æ‹Ÿå·¥å…·å‡½æ•°
def get_logger(name): return MockLogger()
def track_tokens(**kwargs): print(f"[TOKEN TRACKING] {kwargs}")

# æ¨¡æ‹ŸåŸºç±»
class LLMBase:
    def __init__(self, model_name, base_url, api_key, **kwargs):
        self.model_name = model_name
        self.api_key = api_key
        self.kwargs = kwargs

Message = Dict[str, Any]

# å°† Mock æ³¨å…¥ sys.modulesï¼Œè¿™æ ·ä½ çš„ llm_openai.py å¯¼å…¥æ—¶å°±ä¸ä¼šæŠ¥é”™
# (å‰æï¼šä½ éœ€è¦æŠŠä½ çš„ llm_openai.py å†…å®¹ç¨å¾®è°ƒæ•´ä¸€ä¸‹ï¼Œæˆ–è€…ç¡®ä¿è¿è¡Œç¯å¢ƒèƒ½æ‰¾åˆ°ä¾èµ–)
# å¦‚æœä½ åœ¨å®Œæ•´çš„é¡¹ç›®ç»“æ„ä¸­è¿è¡Œï¼Œè¯·åˆ é™¤ä¸‹é¢çš„ sys.modules æ³¨å…¥ä»£ç 
from unittest.mock import MagicMock
sys.modules['..core'] = MagicMock(get_logger=get_logger)
sys.modules['..utils'] = MagicMock(track_tokens=track_tokens)
sys.modules['.llm_base'] = MagicMock(LLMBase=LLMBase, Message=Message)

# ==========================================
# 2. å¯¼å…¥ä½ çš„ç±»
# ==========================================
# å‡è®¾ä½ çš„æ–‡ä»¶åä¸º llm_openai.pyï¼Œä¸”åœ¨åŒä¸€ç›®å½•ä¸‹
try:
    from src.llm.llm_factory import LLMFactory
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè¯·å°†ä½ çš„ç±»ä»£ç ç›´æ¥ç²˜è´´åˆ°è¿™é‡Œï¼Œè¦†ç›–è¿™ä¸€è¡Œ
    print("âŒ æ— æ³•å¯¼å…¥ OpenAICompatibleLLMï¼Œè¯·ç¡®ä¿æ–‡ä»¶åœ¨åŒä¸€ç›®å½•æˆ–æ‰‹åŠ¨ç²˜è´´ç±»ä»£ç ã€‚")
    sys.exit(1)

# ==========================================
# 3. Narrator æ¨¡æ‹Ÿæµ‹è¯•é€»è¾‘
# ==========================================

# å®šä¹‰ Narrator ä¼šç”¨åˆ°çš„å·¥å…· (Schema)
NARRATOR_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "move_entity",
            "description": "ç§»åŠ¨å½“å‰è§’è‰²åˆ°ç›¸é‚»çš„æˆ¿é—´ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "direction": {
                        "type": "string", 
                        "enum": ["North", "South", "East", "West"],
                        "description": "ç§»åŠ¨çš„æ–¹å‘"
                    }
                },
                "required": ["direction"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_location_view",
            "description": "è·å–å½“å‰ä½ç½®çš„è¯¦ç»†æè¿°ã€‚",
            "parameters": {"type": "object", "properties": {}}
        }
    }
]

async def test_narrator_flow():
    # --- é…ç½® ---
    llm = LLMFactory.get_llm("smart"    )

    # --- åœºæ™¯ 1: çº¯é—²èŠ (æµ‹è¯•æµå¼æ–‡æœ¬) ---
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•åœºæ™¯ 1: çº¯é—²èŠ (Streaming Text)")
    print("="*50)
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯è·‘å›¢ä¸»æŒäºº Narratorã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼Œç®€å•ä»‹ç»ä¸€ä¸‹è¿™ä¸ªæ¨¡ç»„çš„èƒŒæ™¯ã€‚"}
    ]

    full_response = ""
    print("Narrator: ", end="")
    
    # è°ƒç”¨ chat (ä¸ä¼  tools)
    async for chunk in llm.chat(messages):
        if isinstance(chunk, str):
            print(chunk, end="", flush=True)
            full_response += chunk
        elif isinstance(chunk, dict):
            print(f"\n[Unexpected Dict]: {chunk}")
    
    print("\n\nâœ… é—²èŠæµ‹è¯•å®Œæˆã€‚")

    # --- åœºæ™¯ 2: æ„å›¾è¯†åˆ« (æµ‹è¯• Function Calling) ---
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•åœºæ™¯ 2: å·¥å…·è°ƒç”¨ (Tool Calling)")
    print("="*50)

    # æ¨¡æ‹Ÿç”¨æˆ·æƒ³è¦ç§»åŠ¨
    messages.append({"role": "assistant", "content": full_response})
    messages.append({"role": "user", "content": "è¿™åœ°æ–¹å¤ªé˜´æ£®äº†ï¼Œæˆ‘è¦å‘åŒ—ç§»åŠ¨ï¼Œç¦»å¼€è¿™é‡Œï¼"})

    print(f"User: {messages[-1]['content']}")
    print("Narrator (Thinking)...")

    tool_calls_received = []
    
    # è°ƒç”¨ chat (ä¼ å…¥ tools)
    async for chunk in llm.chat(messages, tools=NARRATOR_TOOLS):
        
        # æƒ…å†µ A: æ¨¡å‹å¯èƒ½ä¸€è¾¹æ€è€ƒä¸€è¾¹è¯´è¯ (Thinking Process)
        if isinstance(chunk, str):
            # DeepSeek æœ‰æ—¶ä¼šè¾“å‡ºæ€ç»´é“¾å†…å®¹ï¼Œæˆ–è€…ç©ºçš„æ€è€ƒå­—ç¬¦
            print(chunk, end="", flush=True)
            
        # æƒ…å†µ B: æ¨¡å‹å†³å®šè°ƒç”¨å·¥å…· (è¿™æ˜¯ä½ è¦æµ‹è¯•çš„æ ¸å¿ƒ)
        elif isinstance(chunk, dict) and "tool_calls" in chunk:
            tool_calls_received = chunk["tool_calls"]
            print(f"\n\nğŸ› ï¸  æ•æ‰åˆ°å·¥å…·è°ƒç”¨è¯·æ±‚: {json.dumps(tool_calls_received, indent=2, ensure_ascii=False)}")

    # éªŒè¯ç»“æœ
    if tool_calls_received:
        first_call = tool_calls_received[0]
        func_name = first_call['function']['name']
        args = json.loads(first_call['function']['arguments'])
        
        if func_name == "move_entity" and args.get("direction") in ["North", "åŒ—"]:
             print("\nâœ… æµ‹è¯•é€šè¿‡ï¼šæ¨¡å‹æ­£ç¡®è¯†åˆ«äº†ç§»åŠ¨æ„å›¾ã€‚")
        else:
             print(f"\nâš ï¸  æµ‹è¯•å­˜ç–‘ï¼šæ¨¡å‹è°ƒç”¨äº† {func_name} å‚æ•° {args}ï¼Œè¯·æ£€æŸ¥æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šæ¨¡å‹æ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œå®ƒå¯èƒ½ç›´æ¥å›å¤äº†æ–‡æœ¬ã€‚")

if __name__ == "__main__":
    if "sk-your-key-here" in os.getenv("LLM_API_KEY", "sk-your-key-here"):
        print("âš ï¸  è­¦å‘Š: æœªè®¾ç½® LLM_API_KEYï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–ä»£ç ä¸­å¡«å…¥æ­£ç¡®çš„ Keyã€‚")
    
    try:
        asyncio.run(test_narrator_flow())
    except KeyboardInterrupt:
        print("\næµ‹è¯•ç»ˆæ­¢ã€‚")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")